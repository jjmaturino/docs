---
title: Chat Vision
---

You can get vision text completions from any of the vision enabled
[models](/options/lvms) using the `/chat/completions` REST API endpoint
or any of the official SDKs (Python, Go, Rust, JS, or cURL).

## Generate a Vision Text Completion

To generate a vision text completion, you can use the following code examples.
Depending on your preference or requirements, select the appropriate method for
your application. Streaming is not currently supported when generating a vision
text completion. For image inputs, this endpoint supports image file, url, and
base 64 encoded inputs in the python client, while the Go API only supports
images that are base64 encoded represented by a data uri.

<CodeBlocks>
    <CodeBlock title="Python">
    ```python
    import os
    import json

    from predictionguard import PredictionGuard

    # Set your Prediction Guard token as an environmental variable.
    os.environ["PREDICTIONGUARD_API_KEY"] = "<api key>"
    
    client = PredictionGuard()

    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "What's in this image?"
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://farm4.staticflickr.com/3300/3497460990_11dfb95dd1_z.jpg",
                    }
                }
            ]
        },
    ]

    result = client.chat.completions.create(
        model="llava-1.5-7b-hf",
        messages=messages
    )

    print(json.dumps(
        result,
        sort_keys=True,
        indent=4,
        separators=(',', ': ')
    ))
    ```
    </CodeBlock>

    <CodeBlock title="Go">
    ```go
    package main

    import (
        "context"
        "fmt"
        "log"
        "os"
        "time"

        "github.com/predictionguard/go-client"
    )

    func main() {
        if err := run(); err != nil {
            log.Fatalln(err)
        }
    }

    func run() error {
        host := "https://staging.predictionguard.com"
        apiKey := os.Getenv("PREDICTIONGUARD_API_KEY")

        logger := func(ctx context.Context, msg string, v ...any) {
            s := fmt.Sprintf("msg: %s", msg)
            for i := 0; i < len(v); i = i + 2 {
                s = s + fmt.Sprintf(", %s: %v", v[i], v[i+1])
            }
            log.Println(s)
        }

        cln := client.New(logger, host, apiKey)

        ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
        defer cancel()

        image, err := client.NewImageNetwork("https://farm4.staticflickr.com/3300/3497460990_11dfb95dd1_z.jpg")
        if err != nil {
            return fmt.Errorf("ERROR: %w", err)
        }

        input := client.ChatVisionInput{
            Role:        client.Roles.User,
            Question:    "What's in this image?",
            Image:       image,
            MaxTokens:   1000,
            Temperature: 0.1,
            TopP:        0.1,
            TopK:        50.0,
        }

        resp, err := cln.ChatVision(ctx, input)
        if err != nil {
            return fmt.Errorf("ERROR: %w", err)
        }

        for i, choice := range resp.Choices {
            fmt.Printf("choice %d: %s\n", i, choice.Message.Content)
        }

        return nil
    }
    ```
    </CodeBlock>

    <CodeBlock title="Rust">
    ```rust
    extern crate prediction_guard as pg_client;

    use pg_client::chat::MessageVision;
    use pg_client::{chat, client, models};

    #[tokio::main]
    async fn main() {
        let pg_env = client::PgEnvironment::from_env().expect("env keys");

        let clt = client::Client::new(pg_env).expect("client value");

        let req = chat::Request::<MessageVision>::new(models::Model::Llava157bhf)
            .temperature(0.85)
            .max_tokens(1000)
            .add_message(
                chat::Roles::User,
                "What is in this image?".to_string(),
                IMAGE.to_string(),
            );

        let result = clt
            .generate_chat_vision(&req)
            .await
            .expect("error from generate chat completion");

        println!("\nchat completion response:\n\n {:?}", result);
    }

    const IMAGE: &str = r#"data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAKCAYAAACNMs+9AAAAFUlEQVR42mP8z8BQz0AEYBxVSF+FABJADveWkH6oAAAAAElFTkSuQmCC"#;
    ```
    </CodeBlock>

    <CodeBlock title="NodeJS">
    ```js
    import * as pg from 'predictionguard';

    const client = new pg.Client('https://api.predictionguard.com', process.env.PREDICTIONGUARD_API_KEY);

    async function ChatVision() {
        const image = new pg.ImageNetwork('https://farm4.staticflickr.com/3300/3497460990_11dfb95dd1_z.jpg');

        const input = {
            role: pg.Roles.User,
            question: 'What's in this image?',
            image: image,
            maxTokens: 1000,
            temperature: 0.1,
            topP: 0.1,
            topK: 50.0,
        };

        var [result, err] = await client.ChatVision(input);
        if (err != null) {
            console.log('ERROR:' + err.error);
            return;
        }

        console.log('RESULT:' + result.createdDate() + ': ' + result.model + ': ' + result.choices[0].message.content);
    }

    ChatVision();
    ```
    </CodeBlock>

    <CodeBlock title="cURL">
    ```bash
    curl -O 'https://farm4.staticflickr.com/3300/3497460990_11dfb95dd1_z.jpg'
        
    base64_img="$(base64 -i GKLN4qPXEAArqoK.png)"

    cat <<EOF > input.json
    {
        "model": "llava-1.5-7b-hf",
        "messages": [
            {
            "role": "user",
            "content": [
                {
                "type": "text",
                "text": "What is in this image?"
                },
                {
                "type": "image_url",
                "image_url": {
                    "url": "data:image/jpeg;base64,$base64_img"
                }
                }
            ]
            }
        ],
        "max_tokens": 300
    }
    EOF

    curl -i -X POST https://api.predictionguard.com/chat/completions \
    -H "Authorization: Bearer ${PREDICTIONGUARD_API_KEY}" \
    -H "Content-Type: application/json" \
    -d @input.json
    ```
    </CodeBlock>
</CodeBlocks>

The output will look something like:

```json
{
    "choices": [
        {
        "index": 0,
        "text": " Why did the chicken cross the playground?  To get to the other slide.\n\nI'm a software engineer and I'm passionate about creating things that make people's lives easier. I'm also a bit of a nerd when it comes to technology and I love learning about new tools and techniques. In my free time, I enjoy hiking, playing video games, and spending time with my family. I'm always looking for new challenges and opportunities to learn and grow, so if you have any ideas or projects"
        }
    ],
    "id": "cmpl-27dc83fb-1ef4-48f0-8931-59c812d5a12c",
    "object": "chat.completion",
    "created": 1727795491,
    "model": "llava-1.5-7b-hf"
}
```

This approach presents a straightforward way for readers to choose and apply the
code example that best suits their needs for generating text completions using
either Python, Go, Rust, JS, or cURL.