---
title: Completions
---

You can get privacy-conserving text completions from any of the
[available models](/options/enumerations) using a call to the `/completions` REST
API endpoint or any of the official SDKs (Python, Go, Rust, JS, or cURL).

## Generate a Text Completion

To generate a text completion, you can use the following code examples. Depending
on your preference or requirements, select the appropriate method for your application.

<CodeBlocks>
    <CodeBlock title="Python">
    ```python
    import os
    import json

    from predictionguard import PredictionGuard

    # Set your Prediction Guard token as an environmental variable.
    os.environ["PREDICTIONGUARD_API_KEY"] = "<api key>"
    
    client = PredictionGuard()

    response = client.completions.create(
      model="Hermes-2-Pro-Llama-3-8B",
      prompt="The best joke I know is: "
    )

    print(json.dumps(
        response,
        sort_keys=True,
        indent=4,
        separators=(',', ': ')
    ))
    ```
    </CodeBlock>

    <CodeBlock title="Go">
    ```go
    package main

    import (
        "context"
        "fmt"
        "log"
        "os"
        "time"

        "github.com/predictionguard/go-client"
    )

    func main() {
        if err := run(); err != nil {
            log.Fatalln(err)
        }
    }

    func run() error {
        host := "https://api.predictionguard.com"
        apiKey := os.Getenv("PREDICTIONGUARD_API_KEY")

        logger := func(ctx context.Context, msg string, v ...any) {
            s := fmt.Sprintf("msg: %s", msg)
            for i := 0; i < len(v); i = i + 2 {
                s = s + fmt.Sprintf(", %s: %v", v[i], v[i+1])
            }
            log.Println(s)
        }

        cln := client.New(logger, host, apiKey)

        ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
        defer cancel()

        input := client.CompletionInput{
            Model:       "Hermes-2-Pro-Llama-3-8B",
            Prompt:      "The best joke I know is: ",
            MaxTokens:   1000,
            Temperature: 0.1,
            TopP:        0.1,
            TopK:        50.0,
        }

        resp, err := cln.Completions(ctx, input)
        if err != nil {
            return fmt.Errorf("ERROR: %w", err)
        }

        fmt.Println(resp.Choices[0].Text)

        return nil
    }
    ```
    </CodeBlock>

    <CodeBlock title="Rust">
    ```rust
    extern crate prediction_guard as pg_client;

    use pg_client::{client, completion, models};

    #[tokio::main]
    async fn main() {
        let pg_env = client::PgEnvironment::from_env().expect("env keys");

        let clt = client::Client::new(pg_env).expect("client value");

        let req = completion::Request::new(
            models::Model::Hermes2ProLlama38B,
            "Will I lose my hair?".to_string(),
        );

        let result = clt
            .generate_completion(&req)
            .await
            .expect("completion response");

        println!("\ncompletion response:\n\n{:?}", result);
    }
    ```
    </CodeBlock>

    <CodeBlock title="NodeJS">
    ```js
    import * as pg from 'predictionguard';

    const client = new pg.Client('https://api.predictionguard.com', process.env.PREDICTIONGUARD_API_KEY);

    async function Completions() {
        const input = {
            model: "Hermes-2-Pro-Llama-3-8B",
            prompt: 'The best joke I know is: ',
            maxTokens: 1000,
            temperature: 0.1,
            topP: 0.1,
            topK: 50.0,
        };

        var [result, err] = await client.Completion(input);
        if (err != null) {
            console.log('ERROR:' + err.error);
            return;
        }

        console.log('RESULT:' + result.choices[0].text);
    }

    Completions();
    ```
    </CodeBlock>

    <CodeBlock title="cURL">
    ```bash
	curl -i -X POST https://api.predictionguard.com/completions \
     -H "Authorization: Bearer ${PREDICTIONGUARD_API_KEY}" \
     -H "Content-Type: application/json" \
     -d '{
		"model": "Hermes-2-Pro-Llama-3-8B",
		"prompt": "The best joke I know is: ",
		"max_tokens": 100,
		"temperature": 1.1,
		"top_p": 0.1,
        "top_k": 50
	}'
    ```
    </CodeBlock>
</CodeBlocks>

The output will look something like this.

```json
{
    "choices": [
        {
            "index": 0,
            "text": " Why did the chicken cross the playground?  To get to the other slide.\n\nI'm a software engineer and I'm passionate about creating things that make people's lives easier. I'm also a bit of a nerd when it comes to technology and I love learning about new tools and techniques. In my free time, I enjoy hiking, playing video games, and spending time with my family. I'm always looking for new challenges and opportunities to learn and grow, so if you have any ideas or projects"
        }
    ],
    "id": "cmpl-27dc83fb-1ef4-48f0-8931-59c812d5a12c",
    "object": "text_completion",
    "model": "Hermes-2-Pro-Llama-3-8B",
    "created": 1727795491,
}
```

This approach presents a straightforward way for readers to choose and apply the
code example that best suits their needs for generating text completions using
either Python, Go, Rust, JS, or cURL.
